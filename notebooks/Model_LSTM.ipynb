{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10277331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df = pd.read_csv(\"../local_datasets/mars_weather_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79342ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df = mars_weather_df[\n",
    "    [\"terrestrial_date\", \"sol\", \"min_temp\", \"max_temp\", \"pressure\", \"min_gts_temp\", \"max_gts_temp\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a17256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mars_weather_df.shape)\n",
    "mars_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ce26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df[\"terrestrial_date\"] = pd.to_datetime(mars_weather_df[\"terrestrial_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df = mars_weather_df[\n",
    "    (mars_weather_df.terrestrial_date > \"2013-06-01\") & (mars_weather_df.terrestrial_date < \"2018-06-01\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ddcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df.terrestrial_date.min(), mars_weather_df.terrestrial_date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d55a87",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- Sin and cosing transformation of time of year to encode cyclical nature of the data into a feature used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_df[\"timestamp\"] = mars_weather_df.terrestrial_date.apply(lambda x: x.timestamp())\n",
    "\n",
    "num_seconds_in_an_earth_year = 60 * 60 * 24 * 365.25\n",
    "\n",
    "sin_transformation = lambda x: np.sin(\n",
    "    (2 * np.pi * x) / num_seconds_in_an_earth_year\n",
    ")\n",
    "\n",
    "cos_transformation = lambda x: np.cos(\n",
    "    (2 * np.pi * x) / num_seconds_in_an_earth_year\n",
    ")\n",
    "\n",
    "mars_weather_df[\"timestamp_sin\"] = mars_weather_df.timestamp.apply(sin_transformation)\n",
    "mars_weather_df[\"timestamp_cos\"] = mars_weather_df.timestamp.apply(cos_transformation)\n",
    "\n",
    "mars_weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fe40e",
   "metadata": {},
   "source": [
    "## Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(data_df, train_prop): \n",
    "    test_prop = 1 - train_prop\n",
    "    train_data_df = data_df.iloc[:int(len(data_df) * train_prop)]\n",
    "    test_data_df = data_df.iloc[len(train_data_df):]\n",
    "    \n",
    "    return train_data_df, test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train_data, test_data, scaling):\n",
    "    if scaling == \"Standardise\": \n",
    "        train_mean = train_data.mean()\n",
    "        train_std = train_data.std()\n",
    "        \n",
    "        train_data_scaled = (train_data - train_mean) / train_std\n",
    "        test_data_scaled = (test_data - train_mean) / train_std\n",
    "        \n",
    "    if scaling == \"Normalise\": \n",
    "        train_min = train_data.min()\n",
    "        train_max = train_data.max()\n",
    "        \n",
    "        train_data_scaled = (train_data - train_min) / (train_max - train_min)\n",
    "        test_data_scaled = (test_data - train_min) / (train_max - train_min)\n",
    "\n",
    "    \n",
    "    return train_data_scaled, test_data_scaled        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_sequences_and_labels(df, quantity, lags, num_timesteps_to_forecast): \n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    \n",
    "    df_reset_index = df.reset_index(drop=True)\n",
    "    quantity_series = df_reset_index[quantity].values\n",
    "    \n",
    "    for i in range(len(quantity_series) - lags - num_timesteps_to_forecast): \n",
    "        seq_x = df_reset_index.iloc[i:(i + lags)].values\n",
    "        seq_y = quantity_series[(i + lags):(i + lags + num_timesteps_to_forecast)]\n",
    "        \n",
    "        train_x.append(seq_x)\n",
    "        train_y.append(seq_y)\n",
    "        \n",
    "    return np.array(train_x), np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_to_forecast = \"min_temp\"\n",
    "features = [\"timestamp_sin\", \"timestamp_cos\"]\n",
    "mars_weather_df_forecast = mars_weather_df[[quantity_to_forecast] + features]\n",
    "mars_weather_df_forecast.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_df, test_df = create_train_test_split(mars_weather_df_forecast, train_prop=0.7)\n",
    "\n",
    "val_df, test_df = create_train_test_split(test_df, train_prop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = construct_training_sequences_and_labels(\n",
    "    train_df, quantity_to_forecast, lags=10, num_timesteps_to_forecast=1\n",
    ")\n",
    "\n",
    "val_x, val_y = construct_training_sequences_and_labels(\n",
    "    val_df, quantity_to_forecast, lags=10, num_timesteps_to_forecast=1\n",
    ")\n",
    "\n",
    "test_x, test_y = construct_training_sequences_and_labels(\n",
    "    test_df, quantity_to_forecast, lags=10, num_timesteps_to_forecast=1\n",
    ")\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "seq_len, num_features = train_x.shape[1], train_x.shape[2]\n",
    "\n",
    "train_x_reshape = train_x.reshape((train_x.shape[0]*seq_len, num_features))\n",
    "val_x_reshape = val_x.reshape((val_x.shape[0]*seq_len, num_features))\n",
    "test_x_reshape = test_x.reshape((test_x.shape[0]*seq_len, num_features))\n",
    "\n",
    "scaler.fit(train_x_reshape)\n",
    "\n",
    "\n",
    "train_x_scaled = scaler.transform(train_x_reshape).reshape((train_x.shape[0], seq_len, num_features))\n",
    "val_x_scaled = scaler.transform(val_x_reshape).reshape((val_x.shape[0], seq_len, num_features))\n",
    "test_x_scaled = scaler.transform(test_x_reshape).reshape((test_x.shape[0], seq_len, num_features))\n",
    "\n",
    "# scaler_y = MinMaxScaler()\n",
    "# scaler_y.fit(train_y)\n",
    "# train_y_scaled, val_y_scaled, test_y_scaled = scaler_y.transform(train_y), scaler_y.transform(val_y), scaler_y.transform(test_y)\n",
    "\n",
    "                                                                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor = torch.tensor(train_x_scaled, dtype=torch.float)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float)\n",
    "\n",
    "val_x_tensor = torch.tensor(val_x_scaled, dtype=torch.float)\n",
    "val_y_tensor = torch.tensor(val_y, dtype=torch.float)\n",
    "\n",
    "test_x_tensor = torch.tensor(test_x_scaled, dtype=torch.float)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078db28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(train_x_tensor, train_y_tensor)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee954605",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1181965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module): \n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(self.input_size, self.hidden_size, self.n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        output, h_0 = self.rnn(x, h_0.detach())\n",
    "        \n",
    "        output = output[:, -1, :]\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module): \n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        output, (h_0, c_0) = self.lstm(x, (h_0.detach(), c_0.detach()))\n",
    "        \n",
    "        output = output[:, -1, :]\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_x_tensor.size(-1)\n",
    "output_size = 1\n",
    "hidden_size = 64\n",
    "n_layers = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "rnn_model = RNNModel(input_size, output_size, hidden_size, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.AdamW(rnn_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "training_loss_over_epochs = []\n",
    "validation_loss_over_epochs = []\n",
    "\n",
    "best_val_loss = 1000000\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    running_loss = 0\n",
    "    for batch, (train_in, train_out) in enumerate(trainloader): \n",
    "        model_preds = rnn_model(train_in)\n",
    "        \n",
    "        loss = criterion(model_preds, train_out)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    training_loss_over_epochs.append(running_loss/len(trainloader))\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        rnn_model.eval()\n",
    "        \n",
    "        val_preds = rnn_model(val_x_tensor)\n",
    "        val_loss = criterion(val_preds, val_y_tensor)\n",
    "        validation_loss_over_epochs.append(val_loss.item())\n",
    "        \n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}]], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss: \n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        \n",
    "    if epoch - best_epoch > 20: \n",
    "        print(\n",
    "            f\"Training stopped after {epoch} epochs. Training loss: {training_loss_over_epochs[-1]}, Validaton loss: {val_loss}\"\n",
    "        )\n",
    "        break\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss_over_epochs, label=\"Train loss\")\n",
    "plt.plot(validation_loss_over_epochs, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions = rnn_model(test_x_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2421f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(test_set_predictions, \"--\", label=\"Predictions\",linewidth=3)\n",
    "plt.plot(test_y_tensor.numpy(), \"-\", label=\"Ground Truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(scaler_y.inverse_transform(test_set_predictions), \"--\", label=\"Predictions\",linewidth=3)\n",
    "plt.plot(scaler_y.inverse_transform(test_y_tensor.numpy()), \"-\", label=\"Ground Truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77580dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = sklearn.metrics.mean_absolute_error(scaler_y.inverse_transform(test_y_tensor.numpy()), scaler_y.inverse_transform(test_set_predictions))\n",
    "mse = sklearn.metrics.mean_squared_error(scaler_y.inverse_transform(test_y_tensor.numpy()), scaler_y.inverse_transform(test_set_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"MAE\": mae, \n",
    "    \"MSE\": mse\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708851c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79f1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
